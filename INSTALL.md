# Domain Adaptation Tools Installation

## Table of Contents
- [Introduction](#introduction)
- [Installation Steps](#installation-steps)
- [Configuration File](#configuration-file)
- [Storage Considerations](#storage-considerations)
- [Pool Data Folder Structure](#pool-data-folder-structure)
- [Example Folder Structures](#example-folder-structures)
- [Testing Installation](#testing-installation)
- [Dependencies](#dependencies)
  - [KenLM](#kenlm)
  - [Tokenizer](#tokenizer)

----
## Introduction
This document provides step by step installation instructions and guides you through the configuration options, storage considerations and optimization settings that can be adjusted to meet the requrements of different systems and users.

### Installation Steps

### KenLM
https://kheafield.com/code/kenlm/

### Tokenizer
Any tokenizer can be used, so long as it is the same tokenizer used for processing both the *Pool Data* and the *Domain Sample Data*.
If you already have the Moses toolkit installed, then this is a good option. You can also use the XXXXX tokenizer from KEN EMAIL
* Moses Toolkit Tokenizer
* KENS EMAIL TOKENIZER


# Requirements 
 
# KenLM:
	Download from here: https://kheafield.com/code/kenlm/

# Moses :
	Download from here : https://github.com/moses-smt/mosesdecoder.git
# Python Libraries
	run: pip install -r requirements.txt


## Configuration File
------------
The configuration file determines constant elements within the processing such as paths and dependency tools for tokenizing and model training. The default `config.json` configuration file is loaded from the same folder location as the running script. If you wish to use different *Pool Data* or different tokenizers at different times, you can override the default config file using the `-c` argument. 

**Default config.json**
```json
{
	"TokenizerCMD" : "/tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l %lang  -threads 4 < %input_file_path >  %output_file_path ",
	"KenLMPath" : "/tools/kenlm/",
	"KenLMnGram" : "5",
	"KenLMBinarization" : "/tools/kenlm/build_binary -p 1.5 probing %arpa_file_path %binary_file_path",
	"PoolDataRootPath" : "/data/pool/",
	"ScoringMode" : "2"
}
```

*Parameters*
- `-TokenizerCMD` The full path to the tokenizer to be used. The variable names that can be passed through the tools to the tokenizer are as follows:
  - `%lang` - The tokenization language.
  - `%input_file_path` - The path to the input file that is to be tokenized.
  - `%output_file_path` - The path to the tokenized output file.
- `KenLMPath` - The path to the KenLM Installation.
- `KenLMnGram` - The number of nGrams to train the language models with.
- `KenLMBinarization` - The command to binarize the language mode. Set the parameters as per the KenLM documentaiton for your preferences.
  - `%arpa_file_path` - The path to the ARPA input file that is to be binarized. This is generated by the workflow.
  - `%binary_file_path` - The path to the binarized output file. This is generated by the workflow.
- `PoolDataRootPath` - The path to where the *Pool Data* is stored. 
- `ScoringMode` - 1 = Simple using domain sample source language model only, 2 = Moore-Lewis using source language.

>**Note:**
>
>The default configuration file will be loaded automatically by the tools. This file resides in the same folder as the scripts are running. The default configuration file can be overridden by specifing the `-c` argument on any of the tools and providing a path to an alternate configuration file.

**Example Configuration File**
The below example has all supporting tools such as KenLM and the Moses tokenizer installed in the `/tools` folder and the *Pool Data* stored in the `/data/pool/` folder. 5 grams are used for the language model and Moore-Lewis scoreing will be used to score data.

```json
{
	"TokenizerCMD" : "/tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l %lang  -threads 4 < %input_file_path >  %output_file_path ",
	"KenLMPath" : "/tools/kenlm/",
	"KenLMnGram" : "5",
	"KenLMBinarization" : "/tools/kenlm/build_binary -p 1.5 probing %arpa_file_path %binary_file_path",
	"PoolDataRootPath" : "/data/pool/",
	"ScoringMode" : "2"
}
```

## Storage Considerations

Data used in this process can become very large. The initial files, especially the *Pool Data* files can be large from the outset.
* All source data when tokenzied will more than double in size.
* Depending on the size of your *Pool Data*, the working files and generated models can be many gigabytes.
* The extracted domain data can be very large. Depending on the amount of matching data, it can be up to total size of the combined source language and target language pool data. 

**Example Data Sizes**

| Data Source  | Size | Note |
| ------------- | ------------- | ------ |
| Domain Sample - EN Source  | 1 MB  | |
| Domain Sample - EN Source Tokenzied  | 1.1 MB  | Usually about 5-10% larger than the non-tokenized data |
| Domain Sample - EN Source Model  | XXX MB  | Model size will vary depending on the amount of ngram repetition |
| EN-DE Pool Data - EN Source  | 5 GB | Source and target pool data sizes could differ notably |
| EN-DE Pool Data - DE Target  | 5 GB | "    " |
| EN-DE Pool Data - EN Source Tokenized  | 5.5 GB | Usually about 5-10% larger than the non-tokenized data |
| EN-DE Pool Data - EN Source Model  | XXX GB | Model size will vary depending on the amount of ngram repetition |
| EN-DE Domain Match Scores | 0.2 GB | Data is small as it is a single score per line in the Pool Data |
| EN-DE Domain Extract Data - EN Source | 2 GB | Data size will vary depending on how well the sample matches to the pool. The maximum size of the output is the size of the combined source and target language data in the pool data |
| EN-DE Domain Extract Data - DE Target | 2 GB | "    " |

## Pool Data Folder Structure
The *Pool Data* follows a simple structure. Files are stored grouped by language pair and then split into each individual language. This is the same format that ParaCrawl is published in. Each file has 1 sentence per line.

* `{pool_data_path}/{source_language}_{target_language}/` - The root folder for the language pair for the overall pool
* `{pool_data_path}/{source_language}_{target_language}/{source_language}` - The pool files in the source language.
* `{pool_data_path}/{source_language}_{target_language}/{target_language}` - The pool files in the target language. The file names should be identical to the source language file name.
* `{pool_data_path}/{source_language}_{target_language}/{source_language}/tok/` - The tokenized source language data. Each file in the parent source language folder has a tokenized partner in this folder.
* `{pool_data_path}/{source_language}_{target_language}/{source_language}/model/` - The language model trained on the source language tokenized data when Moore-Lewis is used.
**Example:**

```sh
/data/pool/en_de/en/myfile.txt
/data/pool/en_de/en/tok/myfile.txt
/data/pool/en_de/en/model/en_de_pool_lm.arpa
/data/pool/en_de/en/model/en_de_pool_lm.binary
/data/pool/en_de/de/myfile.txt
```

>**Note:**
>
>*Pool Data* and extracted data can be very large. When the *Pool Data* is tokenized, the tokenized data will be at a little bigger than the non-tokenized data due to the spaces added. Ensure that there is enough storage capacity available for this large set of data. See [Storage Considerations](#storage-considerations) above.

### Example Folder Structures
```sh
Script : folder for all .py script with default config file
|
Pool
|___SourceLang_TargetLang
|		|__SourceLang
|		|	|__tok
|		|	|__model
|               |              
|		|__TargetLang
Data
|__domain
|	|__DomainName
|	|__SourceLang_TargetLang
|		|__model
|		|__scores
|		|__0.5
|
|__ mysample
|__ mysample/__tok
```
## Testing Installation
